{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "add400fd-a3d6-436d-a8da-e2b6db82c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 获取URL唯一ID，用于识别唯一舆情\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def getUrlParseId(url):\n",
    "    if len(url) == 0 or url.isspace():\n",
    "        return \"\"\n",
    "    if url.startswith('‑_ '):\n",
    "        url = url[3:]\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc\n",
    "    path = parsed_url.path\n",
    "    if path.endswith('/'):\n",
    "        path = path[0:len(path)-1]\n",
    "    query_params = parse_qs(parsed_url.query)   \n",
    "    if \"www.douyin.com\" == domain:\n",
    "        splitPath = path.split('/')\n",
    "        return \"DY_\" + splitPath[len(splitPath)-1]\n",
    "    if \"www.xiaohongshu.com\" == domain or \"xhslink.com\" == domain:\n",
    "        splitPath = path.split('/')\n",
    "        return \"XHS_\" + splitPath[len(splitPath)-1]\n",
    "    if \"cn.club.vmall.com\" == domain:\n",
    "        splitPath = path.split('/')\n",
    "        return \"HW_\" + splitPath[len(splitPath)-1]\n",
    "    if \"weibo.com\" == domain:\n",
    "        splitPath = path.split('/')\n",
    "        if len(splitPath) == 2:\n",
    "            return \"DY_\" + splitPath[len(splitPath)-1]\n",
    "        return \"WB_\" + path\n",
    "    if \"web.vip.miui.com\" == domain:\n",
    "        return \"XM_\" + query_params['postId'][0]\n",
    "    if \"www.xiaomi.cn\" == domain:\n",
    "        splitPath = path.split('/')\n",
    "        return \"XM_\" + splitPath[len(splitPath)-1]\n",
    "    if \"www.toutiao.com\" == domain:\n",
    "        splitPath = path.split('/')\n",
    "        id = splitPath[len(splitPath)-1]\n",
    "        if id.startswith('i'):\n",
    "            return \"TT_\" + id[1:]\n",
    "        return \"TT_\" + id\n",
    "    if \"toutiao.com\" == domain:\n",
    "        splitPath = path.split('/')\n",
    "        return \"TT_\" + splitPath[len(splitPath)-1]\n",
    "    if \"www.kuaishou.com\" == domain:\n",
    "        splitPath = path.split('/')\n",
    "        return \"KS_\" + splitPath[len(splitPath)-1]\n",
    "    if \"bbs.vivo.com.cn\" == domain:\n",
    "        splitPath = path.split('/')\n",
    "        return \"VIVO_\" + splitPath[len(splitPath)-1]\n",
    "    if \"www.zhihu.com\" == domain:\n",
    "        splitPath = path.split('/')\n",
    "        return \"ZH_\" + splitPath[len(splitPath)-1]\n",
    "    if \"club.hihonor.com\" == domain:\n",
    "        splitPath = path.split('/')\n",
    "        return \"HONOR_\" + splitPath[len(splitPath)-1]\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "791cbb03-be38-49ee-b88e-18dc5b23a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 爬虫爬取舆情：excel，数据格式：发布日期（2024-06-04），渠道（媒体/论坛/应用商店），媒体（华为+荣耀、小米、今日头条等），舆情URL，舆情内容\n",
    "spider_file = \"/Users/lizhiyang03/Desktop/爬虫-外部舆情.xlsx\"\n",
    "\n",
    "## 人工搜索舆情：excel，数据格式：日期（2024-06-04），类别（媒体/论坛/应用商店），媒体（华为+荣耀、小米、今日头条等），URL，内容\n",
    "people_file = \"/Users/lizhiyang03/Desktop/人工-外部舆情.xlsx\"\n",
    "\n",
    "## 结果输出文件：csv格式，数据格式：日期（2024-06-24），类别，媒体，URL，内容，分类\n",
    "output_file = \"/Users/lizhiyang03/Desktop/最终结果-W22.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f9392ecc-580f-484e-b050-22ff5745c659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['发布日期', '渠道', '媒体', '舆情URL', '舆情内容', '舆情详细信息']\n"
     ]
    }
   ],
   "source": [
    "## 解析爬虫爬取舆情数据\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "spider_data = {}\n",
    "df = pd.read_excel(spider_file)\n",
    "columns = df.columns.values.tolist()\n",
    "print(columns)\n",
    "for idx, row in df.iterrows():\n",
    "    date = row['发布日期']\n",
    "    category = row['渠道']\n",
    "    media = row['媒体']\n",
    "    url = row['舆情URL']\n",
    "    content = row['舆情内容']\n",
    "\n",
    "    if type(url) == float:\n",
    "        url = ''\n",
    "    if type(content) == float:\n",
    "        content = ''\n",
    "    if media == '华为' or media == '荣耀':\n",
    "        media = '华为+荣耀'\n",
    "    \n",
    "    id = content\n",
    "    if category != '应用商店':\n",
    "        id = getUrlParseId(url)\n",
    "    spider_data[id] = ','.join((date, category,media,url,content))\n",
    "# print(spider_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b8a82fd3-cfa5-4263-ad29-5cdd160c6854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['category', 'media', 'url', 'content', 'date']\n"
     ]
    }
   ],
   "source": [
    "## 解析人工搜索舆情数据\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "people_data = {}\n",
    "df = pd.read_excel(people_file)\n",
    "columns = df.columns.values.tolist()\n",
    "print(columns)\n",
    "for idx, row in df.iterrows():\n",
    "    date = row['date']\n",
    "    category = row['category']\n",
    "    media = row['media']\n",
    "    url = row['url']\n",
    "    content = row['content']\n",
    "    if type(date) == pd.Timestamp:\n",
    "        date = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    if type(url) == float:\n",
    "        url = ''\n",
    "    if type(content) == float:\n",
    "        content = ''\n",
    "    if media == '华为' or media == '荣耀':\n",
    "        media = '华为+荣耀'\n",
    "    \n",
    "    id = content\n",
    "    if category != '应用商店':\n",
    "        id = getUrlParseId(url)\n",
    "    people_data[id] = ','.join((date, category,media,url,content))\n",
    "# print(people_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4844de57-2068-45de-800e-fa41b7c31399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    }
   ],
   "source": [
    "## 判断两个舆情是否重复\n",
    "result = []\n",
    "for key,value in people_data.items():\n",
    "    if key in spider_data:\n",
    "        result.append(value+',都找到')\n",
    "    else:\n",
    "        result.append(value+',人工搜索')\n",
    "        \n",
    "for key, value in spider_data.items():\n",
    "    if key not in people_data:\n",
    "        result.append(value+',爬虫+人工标注')\n",
    "print(len(result))\n",
    "with open(output_file, 'w') as f:\n",
    "    for item in result:\n",
    "        f.write(item)\n",
    "        f.write('\\r\\n')\n",
    "    f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17020a1-8d72-441b-9819-f91cd561d9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b3074-80ad-4e2d-a8ec-7df0abf2908d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
